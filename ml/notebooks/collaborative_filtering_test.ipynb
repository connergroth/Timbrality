{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with NMF - Testing Notebook\n",
    "\n",
    "This notebook tests the NMF-based collaborative filtering implementation for the Timbrality music recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the project to Python path\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "# Import our custom modules\n",
    "from timbral.models.nmf_model import NMFModel\n",
    "from timbral.utils.data_loader import DataLoader\n",
    "from timbral.logic.trainer import ModelTrainer\n",
    "from timbral.utils.redis_connector import RedisConnector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Data\n",
    "\n",
    "Let's create some synthetic user-item interaction data to test our collaborative filtering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic user-item interaction data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_users = 100\n",
    "n_items = 200\n",
    "n_interactions = 2000\n",
    "\n",
    "# Generate random interactions\n",
    "user_ids = np.random.randint(0, n_users, n_interactions)\n",
    "item_ids = np.random.randint(0, n_items, n_interactions)\n",
    "ratings = np.random.uniform(1, 5, n_interactions)  # Ratings between 1 and 5\n",
    "\n",
    "# Create DataFrame\n",
    "interactions_df = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'item_id': item_ids,\n",
    "    'rating': ratings\n",
    "})\n",
    "\n",
    "# Remove duplicates and keep highest rating for each user-item pair\n",
    "interactions_df = interactions_df.groupby(['user_id', 'item_id'])['rating'].max().reset_index()\n",
    "\n",
    "print(f\"Generated {len(interactions_df)} unique user-item interactions\")\n",
    "print(f\"Users: {interactions_df['user_id'].nunique()}\")\n",
    "print(f\"Items: {interactions_df['item_id'].nunique()}\")\n",
    "print(f\"Sparsity: {1 - len(interactions_df) / (n_users * n_items):.3f}\")\n",
    "\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Data Loader and Create User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Create user-item matrix\n",
    "user_item_matrix = data_loader.create_user_item_matrix(interactions_df)\n",
    "\n",
    "print(f\"User-item matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"Matrix sparsity: {(user_item_matrix == 0).sum().sum() / user_item_matrix.size:.3f}\")\n",
    "print(f\"Non-zero entries: {(user_item_matrix > 0).sum().sum()}\")\n",
    "\n",
    "# Display first few rows and columns\n",
    "user_item_matrix.iloc[:10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train NMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Train NMF model with different numbers of components\n",
    "n_components = 20\n",
    "\n",
    "print(f\"Training NMF model with {n_components} components...\")\n",
    "nmf_model = trainer.train_nmf_model(\n",
    "    user_item_matrix=user_item_matrix,\n",
    "    n_components=n_components,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed!\")\n",
    "print(f\"User factors shape: {nmf_model.user_factors.shape}\")\n",
    "print(f\"Item factors shape: {nmf_model.item_factors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions for a few user-item pairs\n",
    "test_users = np.array([0, 1, 2, 5, 10])\n",
    "test_items = np.array([0, 5, 10, 15, 20])\n",
    "\n",
    "predictions = nmf_model.predict(test_users, test_items)\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "for i, (user, item, pred) in enumerate(zip(test_users, test_items, predictions)):\n",
    "    actual = user_item_matrix.iloc[user, item]\n",
    "    print(f\"User {user}, Item {item}: Predicted={pred:.3f}, Actual={actual:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for a specific user\n",
    "user_id = 0\n",
    "top_k = 10\n",
    "\n",
    "recommended_items, scores = nmf_model.get_top_recommendations(user_id, top_k)\n",
    "\n",
    "print(f\"Top {top_k} recommendations for User {user_id}:\")\n",
    "for i, (item, score) in enumerate(zip(recommended_items, scores)):\n",
    "    actual_rating = user_item_matrix.iloc[user_id, item]\n",
    "    print(f\"{i+1}. Item {item}: Score={score:.3f}, Actual Rating={actual_rating:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/test\n",
    "train_interactions, test_interactions = train_test_split(\n",
    "    interactions_df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train interactions: {len(train_interactions)}\")\n",
    "print(f\"Test interactions: {len(test_interactions)}\")\n",
    "\n",
    "# Create train matrix\n",
    "train_matrix = data_loader.create_user_item_matrix(train_interactions)\n",
    "\n",
    "# Train model on train data\n",
    "train_model = NMFModel(n_components=n_components, random_state=42)\n",
    "train_model.fit(train_matrix.values)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_users = test_interactions['user_id'].values\n",
    "test_items = test_interactions['item_id'].values\n",
    "test_ratings = test_interactions['rating'].values\n",
    "\n",
    "# Map to matrix indices\n",
    "test_user_indices = [data_loader.user_to_idx.get(user, -1) for user in test_users]\n",
    "test_item_indices = [data_loader.item_to_idx.get(item, -1) for item in test_items]\n",
    "\n",
    "# Filter out unknown users/items\n",
    "valid_indices = [(i, u, it) for i, (u, it) in enumerate(zip(test_user_indices, test_item_indices)) \n",
    "                 if u >= 0 and it >= 0 and u < train_model.n_users and it < train_model.n_items]\n",
    "\n",
    "if valid_indices:\n",
    "    original_indices, valid_users, valid_items = zip(*valid_indices)\n",
    "    valid_ratings = test_ratings[list(original_indices)]\n",
    "    \n",
    "    predictions = train_model.predict(np.array(valid_users), np.array(valid_items))\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(valid_ratings, predictions))\n",
    "    print(f\"\\nTest RMSE: {rmse:.3f}\")\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(valid_ratings - predictions))\n",
    "    print(f\"Test MAE: {mae:.3f}\")\n",
    "else:\n",
    "    print(\"No valid test samples found (all users/items unknown)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize User and Item Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of embedding values\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# User factors distribution\n",
    "axes[0, 0].hist(nmf_model.user_factors.flatten(), bins=50, alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of User Factor Values')\n",
    "axes[0, 0].set_xlabel('Factor Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Item factors distribution\n",
    "axes[0, 1].hist(nmf_model.item_factors.flatten(), bins=50, alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution of Item Factor Values')\n",
    "axes[0, 1].set_xlabel('Factor Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Heatmap of first 10 users and factors\n",
    "sns.heatmap(nmf_model.user_factors[:10], cmap='viridis', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('User Factors Heatmap (First 10 Users)')\n",
    "axes[1, 0].set_xlabel('Factor Dimension')\n",
    "axes[1, 0].set_ylabel('User ID')\n",
    "\n",
    "# Heatmap of first 10 items and factors\n",
    "sns.heatmap(nmf_model.item_factors[:, :10].T, cmap='viridis', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Item Factors Heatmap (First 10 Items)')\n",
    "axes[1, 1].set_xlabel('Factor Dimension')\n",
    "axes[1, 1].set_ylabel('Item ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = \"models/test_nmf_model.pkl\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "nmf_model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = NMFModel()\n",
    "loaded_model.load(model_path)\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Loaded model components: {loaded_model.n_components}\")\n",
    "print(f\"Loaded model users: {loaded_model.n_users}\")\n",
    "print(f\"Loaded model items: {loaded_model.n_items}\")\n",
    "\n",
    "# Test that loaded model gives same predictions\n",
    "original_pred = nmf_model.predict([0], [0])\n",
    "loaded_pred = loaded_model.predict([0], [0])\n",
    "\n",
    "print(f\"\\nPrediction comparison:\")\n",
    "print(f\"Original model: {original_pred[0]:.6f}\")\n",
    "print(f\"Loaded model: {loaded_pred[0]:.6f}\")\n",
    "print(f\"Difference: {abs(original_pred[0] - loaded_pred[0]):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Redis Caching (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Redis caching (will gracefully handle if Redis is not available)\n",
    "redis_connector = RedisConnector()\n",
    "\n",
    "if redis_connector.redis_client:\n",
    "    print(\"Redis connection successful!\")\n",
    "    \n",
    "    # Test caching recommendations\n",
    "    sample_recommendations = [\n",
    "        {\"item_id\": 1, \"score\": 0.95, \"title\": \"Song A\"},\n",
    "        {\"item_id\": 2, \"score\": 0.87, \"title\": \"Song B\"},\n",
    "        {\"item_id\": 3, \"score\": 0.82, \"title\": \"Song C\"}\n",
    "    ]\n",
    "    \n",
    "    # Cache recommendations\n",
    "    success = redis_connector.set_recommendations(user_id=123, recommendations=sample_recommendations)\n",
    "    print(f\"Caching recommendations: {'Success' if success else 'Failed'}\")\n",
    "    \n",
    "    # Retrieve recommendations\n",
    "    cached_recs = redis_connector.get_recommendations(user_id=123)\n",
    "    print(f\"Retrieved recommendations: {cached_recs}\")\n",
    "    \n",
    "    # Test caching embeddings\n",
    "    sample_embeddings = {\"user_embeddings\": nmf_model.user_factors[:5]}\n",
    "    success = redis_connector.set_embeddings(\"test_embeddings\", sample_embeddings)\n",
    "    print(f\"Caching embeddings: {'Success' if success else 'Failed'}\")\n",
    "    \n",
    "    # Retrieve embeddings\n",
    "    cached_embeddings = redis_connector.get_embeddings(\"test_embeddings\")\n",
    "    if cached_embeddings:\n",
    "        print(f\"Retrieved embeddings shape: {cached_embeddings['user_embeddings'].shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"Redis not available - caching will be disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance with different number of components\n",
    "components_range = [5, 10, 20, 30, 50]\n",
    "rmse_scores = []\n",
    "training_times = []\n",
    "\n",
    "import time\n",
    "\n",
    "for n_comp in components_range:\n",
    "    print(f\"Testing {n_comp} components...\")\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    model = NMFModel(n_components=n_comp, random_state=42)\n",
    "    model.fit(train_matrix.values)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    if valid_indices:\n",
    "        predictions = model.predict(np.array(valid_users), np.array(valid_items))\n",
    "        rmse = np.sqrt(mean_squared_error(valid_ratings, predictions))\n",
    "        rmse_scores.append(rmse)\n",
    "    else:\n",
    "        rmse_scores.append(float('nan'))\n",
    "    \n",
    "    training_times.append(training_time)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(components_range, rmse_scores, 'bo-')\n",
    "ax1.set_xlabel('Number of Components')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_title('Model Performance vs Components')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(components_range, training_times, 'ro-')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Training Time (seconds)')\n",
    "ax2.set_title('Training Time vs Components')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nPerformance Summary:\")\n",
    "for i, n_comp in enumerate(components_range):\n",
    "    print(f\"{n_comp} components: RMSE={rmse_scores[i]:.3f}, Time={training_times[i]:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. ✅ **NMF Model Implementation**: Successfully trains and makes predictions\n",
    "2. ✅ **Data Loading**: Handles user-item interaction data and creates matrices\n",
    "3. ✅ **Recommendations**: Generates top-k recommendations for users\n",
    "4. ✅ **Model Persistence**: Saves and loads models correctly\n",
    "5. ✅ **Evaluation**: Calculates RMSE and MAE metrics\n",
    "6. ✅ **Redis Integration**: Caches recommendations and embeddings\n",
    "7. ✅ **Performance Analysis**: Tests different hyperparameters\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
